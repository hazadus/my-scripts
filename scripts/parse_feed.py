#!/usr/bin/env -S uv run --quiet --script

# /// script
# dependencies = [
#     "beautifulsoup4",
#     "httpx",
#     "lxml",
#     "feedparser",
# ]
# ///
"""
RSS Feed Parser

–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS-—Ñ–∏–¥–æ–≤ –∏ –≤—ã–≤–æ–¥–∞ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç RSS, Atom –∏ –¥—Ä—É–≥–∏–µ XML-—Ñ–æ—Ä–º–∞—Ç—ã —Ñ–∏–¥–æ–≤.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ü–∞—Ä—Å–∏–Ω–≥ RSS, Atom –∏ –¥—Ä—É–≥–∏—Ö XML-—Ñ–æ—Ä–º–∞—Ç–æ–≤
- –í—ã–≤–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ—Å—Ç–∞, —Å—Å—ã–ª–∫–∏, –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ —Ç–∞–π–º–∞—É—Ç–æ–≤
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–æ–∫
- –û—á–∏—Å—Ç–∫–∞ HTML-—Ç–µ–≥–æ–≤ –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ

–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s https://www.macworld.com/feed
  %(prog)s https://www.macstories.net/feed/ --limit 10
  %(prog)s https://daverupert.com/atom.xml --markdown
  %(prog)s https://blog.cassidoo.co/rss.xml --verbose
"""

import argparse
import re
import sys
from datetime import datetime
from typing import Any, Dict, Optional
from urllib.parse import urlparse

import feedparser
import httpx
from bs4 import BeautifulSoup


class RSSParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è RSS –∏ Atom —Ñ–∏–¥–æ–≤."""

    def __init__(self, timeout: float = 30.0, max_retries: int = 2):
        self.timeout = timeout
        self.max_retries = max_retries
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/rss+xml, application/xml, text/xml, application/atom+xml, */*",
            "Accept-Encoding": "gzip, deflate, br",
            "Accept-Language": "en-US,en;q=0.9",
        }

    def parse_feed(self, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç RSS/Atom —Ñ–∏–¥ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ."""
        for attempt in range(self.max_retries + 1):
            try:
                with httpx.Client(
                    timeout=self.timeout, headers=self.headers, http2=False
                ) as client:
                    response = client.get(url)
                    response.raise_for_status()

                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º feedparser –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
                    feed = feedparser.parse(url)

                    if feed.bozo:
                        print(
                            f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –§–∏–¥ {url} —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–∫–∏",
                            file=sys.stderr,
                        )

                    return self._parse_feedparser_result(feed, url)

            except httpx.HTTPStatusError as e:
                if e.response.status_code == 429:
                    print(
                        f"–û—à–∏–±–∫–∞: –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ {url} (429)", file=sys.stderr
                    )
                elif e.response.status_code == 403:
                    print(f"–û—à–∏–±–∫–∞: –î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω –∫ {url} (403)", file=sys.stderr)
                elif e.response.status_code == 404:
                    print(f"–û—à–∏–±–∫–∞: –õ–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ {url} (404)", file=sys.stderr)
                else:
                    print(
                        f"HTTP –æ—à–∏–±–∫–∞ {e.response.status_code} –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}",
                        file=sys.stderr,
                    )
                return {"error": f"HTTP {e.response.status_code}"}

            except (
                httpx.TimeoutException,
                httpx.ConnectTimeout,
                httpx.ReadTimeout,
            ) as e:
                if attempt < self.max_retries:
                    print(
                        f"–¢–∞–π–º–∞—É—Ç –¥–ª—è {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{self.max_retries + 1}...",
                        file=sys.stderr,
                    )
                    continue
                else:
                    print(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ª–µ–Ω—Ç—ã {url}: {e}", file=sys.stderr)
                    return {"error": "Timeout"}

            except httpx.ConnectError as e:
                if attempt < self.max_retries:
                    print(
                        f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{self.max_retries + 1}...",
                        file=sys.stderr,
                    )
                    continue
                else:
                    print(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ {url}: {e}", file=sys.stderr)
                    return {"error": "Connection error"}

            except Exception as e:
                print(
                    f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ª–µ–Ω—Ç—ã {url}: {e}",
                    file=sys.stderr,
                )
                return {"error": str(e)}

        return {"error": "Max retries exceeded"}

    def _parse_feedparser_result(self, feed, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç feedparser."""
        try:
            feed_info = {
                "type": feed.get("version", "Unknown"),
                "url": url,
                "title": feed.feed.get("title", ""),
                "description": feed.feed.get("description", ""),
                "link": feed.feed.get("link", ""),
                "items": [],
            }

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Ñ–∏–¥–∞
            for entry in feed.entries:
                item_data = {
                    "title": entry.get("title", ""),
                    "link": entry.get("link", ""),
                    "description": entry.get("summary", ""),
                    "pub_date": entry.get("published", ""),
                    "guid": entry.get("id", ""),
                    "author": entry.get("author", ""),
                    "category": (
                        entry.get("tags", [{}])[0].get("term", "")
                        if entry.get("tags")
                        else ""
                    ),
                }

                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                if entry.get("published_parsed"):
                    item_data["parsed_date"] = datetime(*entry.published_parsed[:6])
                elif entry.get("updated_parsed"):
                    item_data["parsed_date"] = datetime(*entry.updated_parsed[:6])

                feed_info["items"].append(item_data)

            return feed_info

        except Exception as e:
            return {"error": f"Feed parsing error: {e}"}

    def _parse_rss(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç RSS —Ñ–∏–¥."""
        feed_info = {
            "type": "RSS",
            "url": url,
            "title": "",
            "description": "",
            "link": "",
            "items": [],
        }

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∏–¥–µ
        channel = soup.find("channel")
        if channel:
            feed_info["title"] = self._get_text(channel.find("title"))
            feed_info["description"] = self._get_text(channel.find("description"))
            feed_info["link"] = self._get_text(channel.find("link"))

        # –≠–ª–µ–º–µ–Ω—Ç—ã —Ñ–∏–¥–∞
        items = soup.find_all("item")
        for item in items:
            item_data = {
                "title": self._get_text(item.find("title")),
                "link": self._get_text(item.find("link")),
                "description": self._get_text(item.find("description")),
                "pub_date": self._get_text(item.find("pubDate")),
                "guid": self._get_text(item.find("guid")),
                "author": self._get_text(item.find("author")),
                "category": self._get_text(item.find("category")),
            }

            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            if item_data["pub_date"]:
                item_data["parsed_date"] = self._parse_date(item_data["pub_date"])

            feed_info["items"].append(item_data)

        return feed_info

    def _parse_atom(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç Atom —Ñ–∏–¥."""
        feed_info = {
            "type": "Atom",
            "url": url,
            "title": "",
            "subtitle": "",
            "link": "",
            "items": [],
        }

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∏–¥–µ
        feed = soup.find("feed")
        if feed:
            feed_info["title"] = self._get_text(feed.find("title"))
            feed_info["subtitle"] = self._get_text(feed.find("subtitle"))

            # –°—Å—ã–ª–∫–∞ –Ω–∞ —Å–∞–π—Ç
            link_elem = feed.find("link", rel="alternate")
            if not link_elem:
                link_elem = feed.find("link")
            if link_elem:
                feed_info["link"] = link_elem.get("href", "")

        # –≠–ª–µ–º–µ–Ω—Ç—ã —Ñ–∏–¥–∞
        entries = soup.find_all("entry")
        for entry in entries:
            entry_data = {
                "title": self._get_text(entry.find("title")),
                "link": "",
                "summary": self._get_text(entry.find("summary")),
                "content": self._get_text(entry.find("content")),
                "published": self._get_text(entry.find("published")),
                "updated": self._get_text(entry.find("updated")),
                "author": "",
                "category": "",
            }

            # –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ—Å—Ç
            link_elem = entry.find("link", rel="alternate")
            if not link_elem:
                link_elem = entry.find("link")
            if link_elem:
                entry_data["link"] = link_elem.get("href", "")

            # –ê–≤—Ç–æ—Ä
            author_elem = entry.find("author")
            if author_elem:
                entry_data["author"] = self._get_text(author_elem.find("name"))

            # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
            category_elem = entry.find("category")
            if category_elem:
                entry_data["category"] = category_elem.get("term", "")

            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            if entry_data["published"]:
                entry_data["parsed_date"] = self._parse_date(entry_data["published"])
            elif entry_data["updated"]:
                entry_data["parsed_date"] = self._parse_date(entry_data["updated"])

            feed_info["items"].append(entry_data)

        return feed_info

    def _get_text(self, element) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ XML —ç–ª–µ–º–µ–Ω—Ç–∞."""
        if element is None:
            return ""

        # –£–±–∏—Ä–∞–µ–º HTML-—Ç–µ–≥–∏
        text = str(element)
        text = re.sub(r"<[^>]+>", "", text)
        text = re.sub(r"\s+", " ", text).strip()

        return text

    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É –¥–∞—Ç—ã –≤ datetime –æ–±—ä–µ–∫—Ç."""
        if not date_str:
            return None

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        date_str = date_str.strip()

        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
        date_formats = [
            "%a, %d %b %Y %H:%M:%S %z",  # RFC 822
            "%a, %d %b %Y %H:%M:%S %Z",  # RFC 822 —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∑–æ–Ω—ã
            "%Y-%m-%dT%H:%M:%S%z",  # ISO 8601
            "%Y-%m-%dT%H:%M:%SZ",  # ISO 8601 UTC
            "%Y-%m-%dT%H:%M:%S",  # ISO 8601 –±–µ–∑ –∑–æ–Ω—ã
            "%Y-%m-%d %H:%M:%S",  # –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç
            "%Y-%m-%d",  # –¢–æ–ª—å–∫–æ –¥–∞—Ç–∞
        ]

        for fmt in date_formats:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue

        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
        return None


def print_feed_info(
    feed_data: Dict[str, Any],
    limit: Optional[int] = None,
    markdown: bool = False,
    verbose: bool = False,
) -> None:
    """–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∏–¥–µ."""
    if "error" in feed_data:
        print(f"–û—à–∏–±–∫–∞: {feed_data['error']}")
        return

    if markdown:
        print_feed_markdown(feed_data, limit)
    else:
        print_feed_text(feed_data, limit, verbose)


def print_feed_text(
    feed_data: Dict[str, Any], limit: Optional[int] = None, verbose: bool = False
) -> None:
    """–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∏–¥–µ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."""
    print(f"üì∞ {feed_data['title'] or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'}")
    print(f"üîó {feed_data['url']}")
    if feed_data.get("link"):
        print(f"üåê {feed_data['link']}")
    if feed_data.get("description"):
        print(f"üìù {feed_data['description']}")
    print(f"üìä –¢–∏–ø: {feed_data['type']}")
    print(f"üìÖ –í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {len(feed_data['items'])}")
    print("=" * 60)

    items = feed_data["items"]
    if limit:
        items = items[:limit]

    for i, item in enumerate(items, 1):
        print(f"\n{i}. {item['title'] or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'}")

        if item.get("link"):
            print(f"   üîó {item['link']}")

        if item.get("parsed_date"):
            print(f"   üìÖ {item['parsed_date'].strftime('%Y-%m-%d %H:%M:%S')}")
        elif item.get("pub_date") or item.get("published"):
            date_str = item.get("pub_date") or item.get("published")
            print(f"   üìÖ {date_str}")

        if verbose and (item.get("description") or item.get("summary")):
            desc = item.get("description") or item.get("summary")
            if desc:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ–ø–∏—Å–∞–Ω–∏—è
                if len(desc) > 200:
                    desc = desc[:200] + "..."
                print(f"   üí¨ {desc}")

        if verbose and item.get("author"):
            print(f"   üë§ {item['author']}")

        if verbose and item.get("category"):
            print(f"   üè∑Ô∏è  {item['category']}")


def print_feed_markdown(feed_data: Dict[str, Any], limit: Optional[int] = None) -> None:
    """–í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∏–¥–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown."""
    print(f"# {feed_data['title'] or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'}")
    print()
    print(f"**URL —Ñ–∏–¥–∞:** {feed_data['url']}")
    if feed_data.get("link"):
        print(f"**–°–∞–π—Ç:** {feed_data['link']}")
    if feed_data.get("description"):
        print(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {feed_data['description']}")
    print(f"**–¢–∏–ø:** {feed_data['type']}")
    print(f"**–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤:** {len(feed_data['items'])}")
    print()
    print("---")
    print()

    items = feed_data["items"]
    if limit:
        items = items[:limit]

    for i, item in enumerate(items, 1):
        print(f"## {i}. {item['title'] or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'}")
        print()

        if item.get("link"):
            print(f"[üîó –û—Ç–∫—Ä—ã—Ç—å –ø–æ—Å—Ç]({item['link']})")
            print()

        if item.get("parsed_date"):
            print(
                f"**–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:** {item['parsed_date'].strftime('%Y-%m-%d %H:%M:%S')}"
            )
        elif item.get("pub_date") or item.get("published"):
            date_str = item.get("pub_date") or item.get("published")
            print(f"**–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏:** {date_str}")

        if item.get("description") or item.get("summary"):
            desc = item.get("description") or item.get("summary")
            if desc:
                print()
                print(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {desc}")

        if item.get("author"):
            print(f"**–ê–≤—Ç–æ—Ä:** {item['author']}")

        if item.get("category"):
            print(f"**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {item['category']}")

        print()
        print("---")
        print()


def main():
    parser = argparse.ArgumentParser(
        description="–ü–∞—Ä—Å–µ—Ä RSS-—Ñ–∏–¥–æ–≤ —Å –≤—ã–≤–æ–¥–æ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ—Å—Ç–∞, —Å—Å—ã–ª–∫–∏ –∏ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s https://www.macworld.com/feed
  %(prog)s https://www.macstories.net/feed/ --limit 5
  %(prog)s https://daverupert.com/atom.xml --markdown
  %(prog)s https://blog.cassidoo.co/rss.xml --verbose --limit 10
        """,
    )

    parser.add_argument("url", help="URL RSS-—Ñ–∏–¥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")

    parser.add_argument(
        "--limit", "-l", type=int, help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–≤–æ–¥–∏–º—ã—Ö –ø–æ—Å—Ç–æ–≤"
    )

    parser.add_argument(
        "--markdown",
        "-m",
        action="store_true",
        help="–í—ã–≤–æ–¥–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown",
    )

    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (–∞–≤—Ç–æ—Ä, –∫–∞—Ç–µ–≥–æ—Ä–∏—è, –æ–ø–∏—Å–∞–Ω–∏–µ)",
    )

    parser.add_argument(
        "--timeout",
        type=float,
        default=30.0,
        help="–¢–∞–π–º–∞—É—Ç –¥–ª—è HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 30)",
    )

    parser.add_argument(
        "--retries",
        type=int,
        default=2,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2)",
    )

    args = parser.parse_args()

    # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
    try:
        parsed_url = urlparse(args.url)
        if not parsed_url.scheme or not parsed_url.netloc:
            print("–û—à–∏–±–∫–∞: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL", file=sys.stderr)
            sys.exit(1)
    except Exception:
        print("–û—à–∏–±–∫–∞: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL", file=sys.stderr)
        sys.exit(1)

    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä
    parser = RSSParser(timeout=args.timeout, max_retries=args.retries)

    # –ü–∞—Ä—Å–∏–º —Ñ–∏–¥
    print(f"–ó–∞–≥—Ä—É–∂–∞—é —Ñ–∏–¥: {args.url}")
    feed_data = parser.parse_feed(args.url)

    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print_feed_info(feed_data, args.limit, args.markdown, args.verbose)


if __name__ == "__main__":
    main()
